"""
Module pour d√©couper les fichiers markdown en chunks optimis√©s pour la vectorisation.
Utilise LangChain pour un chunking intelligent qui respecte la structure du document.
"""

import os
from pathlib import Path
from typing import List, Dict

from langchain_text_splitters import RecursiveCharacterTextSplitter


def create_smart_splitter(chunk_size: int = 1000, chunk_overlap: int = 200) -> RecursiveCharacterTextSplitter:
    """
    Cr√©e un splitter intelligent qui respecte la structure markdown.

    Args:
        chunk_size: Taille maximale d'un chunk en caract√®res
        chunk_overlap: Nombre de caract√®res de chevauchement entre chunks

    Returns:
        RecursiveCharacterTextSplitter configur√©
    """
    # S√©parateurs hi√©rarchiques pour markdown (du plus important au moins important)
    separators = [
        "\n### ",  # Titres niveau 3 (sections)
        "\n## ",   # Titres niveau 2
        "\n# ",    # Titres niveau 1
        "\n\n",    # Paragraphes
        "\n",      # Lignes
        ". ",      # Phrases
        " ",       # Mots
        ""         # Caract√®res
    ]

    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=separators,
        is_separator_regex=False,
    )


def chunk_markdown_file(
    file_path: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200
) -> Dict:
    """
    D√©coupe un fichier markdown en chunks avec m√©tadonn√©es.

    Args:
        file_path: Chemin du fichier markdown
        chunk_size: Taille maximale d'un chunk
        chunk_overlap: Overlap entre chunks

    Returns:
        Dictionnaire avec informations sur le fichier et ses chunks
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # Cr√©er le splitter
    splitter = create_smart_splitter(chunk_size, chunk_overlap)

    # D√©couper le texte
    chunks = splitter.split_text(text)

    # Cr√©er les m√©tadonn√©es pour chaque chunk
    chunks_with_metadata = []
    for i, chunk in enumerate(chunks):
        chunks_with_metadata.append({
            'content': chunk,
            'metadata': {
                'source': file_path,
                'file_name': os.path.basename(file_path),
                'chunk_index': i,
                'total_chunks': len(chunks),
                'chunk_size': len(chunk)
            }
        })

    return {
        'file_path': file_path,
        'file_name': os.path.basename(file_path),
        'num_chunks': len(chunks),
        'chunks': chunks_with_metadata,
        'total_chars': len(text)
    }


def chunk_all_markdown_files(
    directory: str = "./OUTPUT",
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    verbose: bool = True
) -> List[Dict]:
    """
    D√©coupe tous les fichiers markdown d'un r√©pertoire en chunks.

    Args:
        directory: R√©pertoire contenant les fichiers markdown
        chunk_size: Taille maximale d'un chunk
        chunk_overlap: Overlap entre chunks
        verbose: Afficher les logs de progression

    Returns:
        Liste de dictionnaires avec les informations de chunking
    """
    output_dir = Path(directory)

    if not output_dir.exists():
        print(f"Le r√©pertoire {directory} n'existe pas.")
        return []

    # Trouver tous les fichiers .md
    md_files = list(output_dir.glob("*.md"))

    if not md_files:
        print(f"Aucun fichier .md trouv√© dans {directory}")
        return []

    if verbose:
        print(f"\nüìù Chunking de {len(md_files)} fichier(s) markdown...")
        print(f"   Param√®tres: chunk_size={chunk_size}, overlap={chunk_overlap}\n")

    results = []

    for i, md_file in enumerate(md_files, start=1):
        if verbose:
            print(f"  [{i}/{len(md_files)}] Traitement de {md_file.name}...")

        result = chunk_markdown_file(str(md_file), chunk_size, chunk_overlap)
        results.append(result)

        if verbose:
            print(f"       ‚úÖ {result['num_chunks']} chunks cr√©√©s")

    if verbose:
        total_chunks = sum(r['num_chunks'] for r in results)
        print(f"\n‚úÖ Chunking termin√©: {total_chunks} chunks au total")

    return results


if __name__ == "__main__":
    print("=== Chunking intelligent des fichiers markdown (LangChain) ===\n")

    # Param√®tres de chunking
    # Note: OpenAI recommande ~500-1000 tokens pour les embeddings
    # 1 token ‚âà 4 caract√®res en fran√ßais
    # Donc 1000 caract√®res ‚âà 250 tokens (taille optimale)
    CHUNK_SIZE = 1000  # Caract√®res
    CHUNK_OVERLAP = 200  # Overlap pour pr√©server le contexte

    results = chunk_all_markdown_files(
        directory="./OUTPUT",
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )

    if not results:
        print("Aucun fichier trait√©.")
    else:
        print(f"Nombre de fichiers trait√©s: {len(results)}\n")

        total_chunks = 0

        for result in results:
            print(f"Fichier: {result['file_name']}")
            print(f"  Taille: {result['total_chars']:,} caract√®res")
            print(f"  Nombre de chunks: {result['num_chunks']}")

            # Afficher les tailles min/max des chunks
            chunk_sizes = [c['metadata']['chunk_size'] for c in result['chunks']]
            if chunk_sizes:
                print(f"  Taille des chunks: min={min(chunk_sizes)}, max={max(chunk_sizes)}, moy={sum(chunk_sizes)//len(chunk_sizes)}")

            print()
            total_chunks += result['num_chunks']

        print(f"Total de chunks g√©n√©r√©s: {total_chunks}")
        print(f"\nParam√®tres de chunking:")
        print(f"  - Taille de chunk: {CHUNK_SIZE} caract√®res (~{CHUNK_SIZE//4} tokens)")
        print(f"  - Overlap: {CHUNK_OVERLAP} caract√®res")
        print(f"  - Splitter: RecursiveCharacterTextSplitter (respecte la structure markdown)")
