# Configuration OpenAI
OPENAI_API_KEY=your_openai_api_key_here

# Configuration Pinecone (https://app.pinecone.io/)
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=pdf-documents
PINECONE_DIMENSION=1536

# Configuration des chemins
DATA_DIR=./DATA
OUTPUT_DIR=./OUTPUT
CACHE_DIR=./.cache

# Configuration du chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
USE_SEMANTIC_CHUNKING=false  # true pour chunking par sections (idéal docs longs)

# Configuration des embeddings
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_BATCH_SIZE=100

# Mistral OCR (optionnel)
MISTRAL_API_KEY=your_mistral_api_key_here
USE_MISTRAL_OCR=false  # true pour activer Mistral OCR
MISTRAL_OCR_FALLBACK=true  # true pour fallback vers Tesseract si erreur


USE_TOKEN_BASED_CHUNKING=true
FILTER_CHUNK_QUALITY=true
MERGE_SMALL_CHUNKS=false
SMART_BATCHING=false

# Performance (Multithreading)
# Nombre maximum de threads pour l'extraction PDF (None = auto: CPU_COUNT + 4)
MAX_WORKERS_PDF_EXTRACTION=
# Nombre maximum de threads pour le chunking (None = auto: CPU_COUNT + 4)
MAX_WORKERS_CHUNKING=
# Nombre maximum de threads pour les embeddings (recommandé: 4 max pour respecter les limites de l'API OpenAI)
MAX_WORKERS_EMBEDDINGS=4
